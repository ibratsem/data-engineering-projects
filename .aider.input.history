
# 2024-12-15 20:44:38.034383
+y

# 2024-12-15 20:45:30.857051
+how to create a branck for airflow here in git

# 2024-12-15 20:45:59.023676
+y

# 2024-12-15 20:45:59.036739
+/run git checkout main

# 2024-12-15 20:45:59.036739
+/run git checkout main

# 2024-12-15 20:46:07.311768
+n

# 2024-12-15 20:46:42.762977
+y

# 2024-12-15 20:46:42.775735
+/run git checkout -b airflow

# 2024-12-15 20:46:42.775735
+/run git checkout -b airflow

# 2024-12-15 20:46:50.513006
+n

# 2024-12-15 20:46:54.201540
+y

# 2024-12-15 20:46:54.213680
+/run git push origin airflow

# 2024-12-15 20:46:54.213680
+/run git push origin airflow

# 2024-12-15 20:47:04.661781
+n

# 2024-12-15 23:49:44.316450
+do i habe a docker compose file?

# 2024-12-15 23:51:08.175340
+check if i have docker compose file

# 2024-12-15 23:51:56.488056
+n

# 2024-12-15 23:52:31.710639
+i want to add some data for airflow for this file can you make changes

# 2024-12-15 23:52:56.209146
+services:                                                                                                                                       
+  postgres:                                                                                                                                     
+    image: postgres:13                                                                                                                         
+    environment:                                                                                                                                
+      POSTGRES_USER: airflow                                                                                                                    
+      POSTGRES_PASSWORD: airflow                                                                                                                
+      POSTGRES_DB: airflow                                                                                                                      
+    volumes:                                                                                                                                    
+      - postgres_data:/var/lib/postgresql/data                                                                                                  
+                                                                                                                                                
+  redis:                                                                                                                                        
+    image: redis:latest                                                                                                                         
+                                                                                                                                                
+  webserver:                                                                                                                                    
+    image: apache/airflow:2.6.3                                                                                                                 
+    environment:
+      AIRFLOW__CORE__EXECUTOR: CeleryExecutor                                                                                                   
+      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflowpostgres/airflow                                                   
+      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflowpostgres/airflow                                                         
+      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0                                                                                         
+      AIRFLOW__CORE__FERNET_KEY: ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=                                                                                              
+      AIRFLOW__CORE__LOAD_EXAMPLES: false                                                                                                      
+    volumes:                                                                                                                                    
+      - ./dags:/opt/airflow/dags                                                                                                                
+    ports:
+      - "8080:8080"
+    depends_on:
+      - init
+    command: webserver                                                                                                                          
+                                                                                                                                                
+  scheduler:                                                                                                                                    
+    image: apache/airflow:2.6.3                                                                                                                 
+    environment:                                                                                                                                
+      AIRFLOW__CORE__EXECUTOR: CeleryExecutor                                                                                                   
+      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflowpostgres/airflow                                                   
+      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflowpostgres/airflow                                                         
+      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0                                                                                         
+      AIRFLOW__CORE__FERNET_KEY: ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=                                                                                              
+      AIRFLOW__CORE__LOAD_EXAMPLES: false                                                                                                      
+    volumes:                                                                                                                                    
+      - ./dags:/opt/airflow/dags                                                                                                                
+    depends_on:                                                                                                                                 
+      - init
+    command: scheduler                                                                                                                          
+                                                                                                                                                
+  init:
+    image: apache/airflow:2.6.3
+    environment:
+      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
+      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflowpostgres/airflow
+      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflowpostgres/airflow
+      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
+      AIRFLOW__CORE__FERNET_KEY: ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=
+      AIRFLOW__CORE__LOAD_EXAMPLES: false
+    volumes:
+      - ./dags:/opt/airflow/dags
+    depends_on:
+      - postgres
+      - redis
+    command: bash -c "airflow db init  airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email adminexample.com"
+    restart: on-failure
+
+  worker:
+    image: apache/airflow:2.6.3
+    environment:
+      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
+      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflowpostgres/airflow
+      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflowpostgres/airflow
+      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
+      AIRFLOW__CORE__FERNET_KEY: ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=
+      AIRFLOW__CORE__LOAD_EXAMPLES: false
+    volumes:
+      - ./dags:/opt/airflow/dags
+    depends_on:
+      - init
+    command: celery worker
+
+volumes:
+  postgres_data:
+

# 2024-12-15 23:54:18.023436
+y

# 2024-12-15 23:56:58.385680
+i dont get what you did. you should add airflow to docker compose

# 2024-12-16 00:10:34.042067
+can we rename docker-compose to docker-compose-kafka

# 2024-12-16 00:12:16.459391
+y

# 2024-12-16 00:12:16.474748
+/run ren docker-compose.yaml docker-compose-kafka.yaml

# 2024-12-16 00:12:16.476208
+/run ren docker-compose.yaml docker-compose-kafka.yaml

# 2024-12-16 00:14:48.032366
+where are we

# 2024-12-16 00:15:21.781508
+yes

# 2024-12-16 00:15:21.796867
+/run ren docker-compose.yaml docker-compose-kafka.yaml

# 2024-12-16 00:15:21.798307
+/run ren docker-compose.yaml docker-compose-kafka.yaml

# 2024-12-16 00:15:24.447926
+n

# 2024-12-16 00:15:35.994056
+push changes 

# 2024-12-16 00:16:04.294429
+y

# 2024-12-16 00:16:04.311822
+/run git add .

# 2024-12-16 00:16:04.312824
+/run git add .

# 2024-12-16 00:16:04.644448
+/run git commit -m "Rename docker-compose.yaml to docker-compose-kafka.yaml and add Airflow services"

# 2024-12-16 00:16:04.644448
+/run git commit -m "Rename docker-compose.yaml to docker-compose-kafka.yaml and add Airflow services"

# 2024-12-16 00:16:05.034436
+/run git push origin main

# 2024-12-16 00:16:05.035419
+/run git push origin main

# 2024-12-16 00:16:10.719428
+n

# 2024-12-16 00:22:07.027295
+i renamed the file in main directory. how to push changes

# 2024-12-16 00:22:49.296521
+y

# 2024-12-16 00:23:17.747633
+/run git add .

# 2024-12-16 00:23:17.748632
+/run git add .

# 2024-12-16 00:23:18.068934
+/run git commit -m "Rename docker-compose.yaml to docker-compose-kafka.yaml and add Airflow services"

# 2024-12-16 00:23:18.070291
+/run git commit -m "Rename docker-compose.yaml to docker-compose-kafka.yaml and add Airflow services"

# 2024-12-16 00:23:18.462250
+/run git push origin main

# 2024-12-16 00:23:18.462250
+/run git push origin main

# 2024-12-16 00:24:26.713937
+n

# 2024-12-16 00:24:37.078349
+now how to get that changes

# 2024-12-16 00:24:56.674318
+y

# 2024-12-16 00:24:56.691798
+/run git fetch origin

# 2024-12-16 00:24:56.692812
+/run git fetch origin

# 2024-12-16 00:24:59.991647
+/run git pull origin main

# 2024-12-16 00:24:59.992627
+/run git pull origin main

# 2024-12-16 00:25:06.759730
+n

# 2024-12-16 00:25:26.794040
+go to airflow branch

# 2024-12-16 00:25:45.835045
+y

# 2024-12-16 00:25:45.849991
+/run git checkout airflow

# 2024-12-16 00:25:45.851426
+/run git checkout airflow

# 2024-12-16 00:25:48.370523
+n

# 2024-12-16 00:25:52.797352
+y

# 2024-12-16 00:25:52.813448
+/run git checkout -b airflow

# 2024-12-16 00:25:52.814483
+/run git checkout -b airflow

# 2024-12-16 00:25:57.268199
+n

# 2024-12-16 00:26:22.390356
+show the changes we made in airflow

# 2024-12-16 00:26:48.946666
+y

# 2024-12-16 00:26:48.966103
+/run git checkout airflow

# 2024-12-16 00:26:48.966103
+/run git checkout airflow

# 2024-12-16 00:26:52.641486
+n

# 2024-12-16 00:26:56.120277
+y

# 2024-12-16 00:26:56.138095
+/run git diff

# 2024-12-16 00:26:56.138095
+/run git diff

# 2024-12-16 00:27:04.634438
+n

# 2024-12-16 00:27:29.634616
+just show the changes in airflow branch

# 2024-12-16 00:28:01.768830
+n

# 2024-12-16 00:29:52.315398
+how to get out of git check

# 2024-12-16 14:55:29.740558
+n

# 2024-12-16 14:55:46.871081
+how to add file to the putty

# 2024-12-16 17:03:58.697450
+n

# 2024-12-16 19:08:09.326270
+go to folder airflow

# 2024-12-16 19:08:43.375241
+n

# 2024-12-16 22:05:20.232464
+go to folder D:\IT\git\data-engineering-projects\airflow\

# 2024-12-16 22:05:41.868198
+y

# 2024-12-16 22:05:41.878399
+/run cd D:\IT\git\data-engineering-projects\airflow\

# 2024-12-16 22:05:41.878910
+/run cd D:\IT\git\data-engineering-projects\airflow\

# 2024-12-16 22:05:45.962842
+n

# 2024-12-16 22:06:06.339187
+ls

# 2024-12-16 22:07:29.289946
+/run cd D:\IT\git\data-engineering-projects\airflow\

# 2024-12-16 22:07:29.290483
+/run cd D:\IT\git\data-engineering-projects\airflow\

# 2024-12-16 22:08:05.698200
+y

# 2024-12-16 22:08:05.706006
+/run dir

# 2024-12-16 22:08:05.707356
+/run dir

# 2024-12-16 22:08:59.822526
+n

# 2024-12-17 23:17:49.746499
+try to run airflow on docker using docker-compose-airflow.yml file

# 2024-12-17 23:17:56.898760
+y

# 2024-12-17 23:18:36.659546
+/run cd D:\IT\git\data-engineering-projects\airflow

# 2024-12-17 23:18:36.659546
+/run cd D:\IT\git\data-engineering-projects\airflow

# 2024-12-17 23:18:41.491153
+y

# 2024-12-17 23:18:41.501772
+/run docker-compose -f docker-compose-airflow.yml up -d

# 2024-12-17 23:18:41.501772
+/run docker-compose -f docker-compose-airflow.yml up -d

# 2024-12-17 23:18:53.481609
+y

# 2024-12-17 23:18:58.222832
+/run docker-compose -f docker-compose-airflow.yml down

# 2024-12-17 23:18:58.223817
+/run docker-compose -f docker-compose-airflow.yml down

# 2024-12-17 23:19:04.007729
+y

# 2024-12-17 23:19:04.020030
+/run cd D:\IT\git\data-engineering-projects\airflow

# 2024-12-17 23:19:04.020544
+/run cd D:\IT\git\data-engineering-projects\airflow

# 2024-12-17 23:19:04.153401
+/run docker-compose -f docker-compose-airflow.yml up -d

# 2024-12-17 23:19:04.154956
+/run docker-compose -f docker-compose-airflow.yml up -d

# 2024-12-17 23:19:35.874548
+y

# 2024-12-17 23:19:48.728515
+so do we succeed?

# 2024-12-17 23:20:21.600764
+y

# 2024-12-17 23:20:21.612559
+/run docker-compose -f docker-compose-airflow.yml up -d

# 2024-12-17 23:20:21.613823
+/run docker-compose -f docker-compose-airflow.yml up -d

# 2024-12-17 23:24:24.573457
+y

# 2024-12-17 23:24:29.019070
+/run docker-compose -f docker-compose-airflow.yml logs

# 2024-12-17 23:24:29.019070
+/run docker-compose -f docker-compose-airflow.yml logs

# 2024-12-17 23:24:34.364985
+y

# 2024-12-17 23:25:29.920984
+n

# 2024-12-18 00:02:23.496064
+find the file demo.py in dags repo

# 2024-12-18 00:08:45.189093
+т

# 2024-12-18 00:08:49.895950
+n

# 2024-12-18 00:08:54.130575
+/clear

# 2024-12-18 00:09:10.829981
+find the file demo.py in dags repo

# 2024-12-18 00:10:06.981793
+check all folders in repo

# 2024-12-18 00:10:43.528367
+n

# 2024-12-18 00:12:50.670315
+/add D:\IT\git\data-engineering-projects\dags\

# 2024-12-18 00:13:11.366320
+/add dags/demo.py

# 2024-12-18 00:13:23.020834
+check the context of the file

# 2024-12-18 00:14:40.855502
+go through demo.py when i ran this dag in airflow it runs multiple times/ why

# 2024-12-18 00:30:40.819481
+n

# 2024-12-18 00:32:46.889718
+how to make it run once in a hour

# 2024-12-18 00:33:53.489643
+n

# 2024-12-18 00:34:40.791098
+i want the same start time but run it hourly

# 2024-12-18 00:35:02.167867
+n

# 2024-12-18 00:35:25.171580
+apply the changes to the dag file

# 2024-12-18 00:35:41.627527
+n

# 2024-12-18 00:36:26.153548
+make it start at 0:40

# 2024-12-18 00:36:48.598263
+n

# 2024-12-18 00:54:03.739047
+*** !!!! Please make sure that all your Airflow components (e.g. schedulers, webservers, workers and triggerer) have the same secret_key configured in webserver section and time is synchronized on all your machines (for example with ntpd)
+See more at https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.htmlsecret-key
+*** Could not read served logs: Client error 403 FORBIDDEN for url http://077de30966c1:8793/log/dag_id=demo/run_id=scheduled__2024-12-17T20:40:00+00:00/task_id=airflow/attempt=1.log
+For more information check: https://httpstatuses.com/403

# 2024-12-18 00:54:08.396851
+n

# 2024-12-18 00:56:14.254814
+where can i take a secret key?

# 2024-12-18 00:56:51.255971
+n

# 2024-12-18 00:57:01.711911
+can i use any key like ibra88

# 2024-12-18 00:57:29.590059
+n

# 2024-12-18 00:58:33.369070
+generate any suitable key and replace

# 2024-12-18 00:59:02.416733
+n

# 2024-12-18 01:01:47.366895
+should we restart webserver now?

# 2024-12-18 01:02:38.496737
+n

# 2024-12-18 01:12:15.155740
+*** Could not read served logs: Client error 403 FORBIDDEN for url http://077de30966c1:8793/log/dag_id=demo/run_id=manual__2024-12-17T22:10:52.213009+00:00/task_id=airflow/attempt=1.log
+For more information check: https://httpstatuses.com/403

# 2024-12-18 01:12:20.060446
+y

# 2024-12-18 01:16:06.545286
+n

# 2024-12-18 01:16:34.815751
+y

# 2024-12-18 01:16:34.823842
+/run docker-compose -f docker-compose-airflow.yml logs webserver

# 2024-12-18 01:16:34.825064
+/run docker-compose -f docker-compose-airflow.yml logs webserver

# 2024-12-18 01:17:08.494643
+/run docker-compose -f docker-compose-airflow.yml logs scheduler

# 2024-12-18 01:17:08.494643
+/run docker-compose -f docker-compose-airflow.yml logs scheduler

# 2024-12-18 01:17:32.293223
+/run docker-compose -f docker-compose-airflow.yml logs worker

# 2024-12-18 01:17:32.293223
+/run docker-compose -f docker-compose-airflow.yml logs worker

# 2024-12-18 01:18:06.535341
+y

# 2024-12-21 16:23:33.355259
+n

# 2024-12-21 16:24:19.972896
+export DEEPSEEK_API_KEY=sk-894e15f3ebce48299458041b1b9e7df6

# 2024-12-21 16:30:38.190371
+i opened docker app how to check if it running

# 2024-12-21 16:30:48.940644
+y

# 2024-12-21 16:30:48.949438
+/run docker info

# 2024-12-21 16:30:48.950440
+/run docker info

# 2024-12-21 16:30:52.976968
+n

# 2024-12-21 16:31:12.513705
+so is it running

# 2024-12-21 16:31:21.460803
+y

# 2024-12-21 16:31:21.470032
+/run docker info

# 2024-12-21 16:31:21.471014
+/run docker info

# 2024-12-21 16:31:25.655187
+y

# 2024-12-21 16:32:23.626989
+so 

# 2024-12-21 16:41:38.142417
+hot ot stop dag in airflow

# 2024-12-21 16:43:43.063296
+n

# 2024-12-21 16:43:54.703029
+worker  is not running

# 2024-12-21 16:45:00.007282
+n

# 2024-12-21 16:45:06.232497
+jwt.exceptions.InvalidSignatureError: Signature verification failed

# 2024-12-21 16:50:53.307103
+т

# 2024-12-21 16:50:55.317057
+N

# 2024-12-21 16:51:23.555368
+can you check docker-compose-airflow file

# 2024-12-21 16:52:36.763064
+/add D:\IT\git\data-engineering-projects\docker-compose-airflow.yml

# 2024-12-21 16:52:45.768263
+ls

# 2024-12-21 16:52:48.076566
+n

# 2024-12-21 16:52:49.664063
+ls

# 2024-12-21 16:53:14.874231
+n

# 2024-12-21 16:53:18.448408
+y

# 2024-12-21 16:53:18.456065
+/run ls

# 2024-12-21 16:53:18.456065
+/run ls

# 2024-12-21 16:53:25.158692
+y

# 2024-12-21 16:53:31.188920
+n

# 2024-12-21 16:53:50.758572
+check docker-compose-airflow file

# 2024-12-21 16:54:06.586780
+add to the chat

# 2024-12-21 16:57:27.858830
+/add /d/it.git/data-engineering-projects/docker-compose-airflow.yml

# 2024-12-21 16:57:55.402893
+n

# 2024-12-21 16:58:03.808093
+how to add file to the chat

# 2024-12-21 17:00:35.740340
+i want to add it like /add

# 2024-12-21 17:00:52.435804
+y

# 2024-12-21 17:00:52.444243
+/run /add docker-compose-airflow.yml

# 2024-12-21 17:00:52.444243
+/run /add docker-compose-airflow.yml

# 2024-12-21 17:01:03.078418
+y

# 2024-12-21 17:01:13.149540
+so

# 2024-12-21 17:02:35.216402
+/add docker-compose-airflow.yml

# 2024-12-21 17:02:43.586254
+check the file

# 2024-12-21 21:11:24.687591
+go through file and check if there are any issues

# 2024-12-21 21:13:12.423048
+/add docker-compose-airflow.yml

# 2024-12-21 21:13:23.115353
+check the file

# 2024-12-21 21:14:02.061129
+you said in previous message that the file is already in the chat

# 2024-12-21 21:15:17.030574
+/add docker-compose-airflow.yml

# 2024-12-21 21:15:50.320754
+/add docker-compose-airflow.yml
+
+D:\IT\git\data-engineering-projects\docker-compose-airflow.yml is already in the chat as an editable file

# 2024-12-21 21:16:04.622199
+n

# 2024-12-21 21:16:08.429075
+nn

# 2024-12-21 21:16:08.835588
+n

# 2024-12-21 21:26:20.856943
+ls

# 2024-12-21 21:26:40.870568
+/run dir

# 2024-12-21 21:26:40.871118
+/run dir

# 2024-12-21 21:28:38.812114
+n

# 2024-12-21 21:28:39.758049
+nn

# 2024-12-21 21:28:39.939147
+n

# 2024-12-21 21:29:06.301094
+n
+how to check all files in the chat

# 2024-12-21 21:29:56.870459
+y

# 2024-12-21 21:29:56.882496
+/run /files

# 2024-12-21 21:29:56.883498
+/run /files

# 2024-12-21 21:30:03.776562
+y

# 2024-12-21 21:30:13.252372
+go on

# 2024-12-21 21:30:26.917387
+y

# 2024-12-21 21:30:26.930590
+/run /files

# 2024-12-21 21:30:26.931118
+/run /files

# 2024-12-21 21:31:00.124466
+y

# 2024-12-21 21:37:48.415708
+
+FROM apache/airflow:latest
+
+apache/airflow:latest: failed to resolve source metadata for docker.io/apache/airflow:latest: failed to authorize: failed to fetch oauth token: Post "https://auth.docker.io/token": read tcp 172.19.0.1:49562->54.227.20.253:443: wsarecv: An existing connection was forcibly closed by the remote host. 
+2
+
+
+3
+
+USER root
+4
+
+RUN apt-get update && \
+5
+
+    apt-get -y install git && \
+6
+
+    apt-get clean
+7
+
+    
+8
+
+USER airflow

# 2024-12-21 21:37:54.221849
+n

# 2024-12-21 21:47:29.758029
+give me the list of the files you have

# 2024-12-21 21:47:52.724360
+/add docker-compose-airflow.yml

# 2024-12-21 21:48:01.299133
+go through the file

# 2024-12-21 21:51:01.329182
+i also have a dag file can you see it

# 2024-12-21 21:51:21.866789
+y

# 2024-12-21 22:18:48.687205
+where is airflow.cfg located?

# 2024-12-21 22:23:51.345471
+i use docker

# 2024-12-21 22:24:39.771790
+printenv | grep AIRFLOW
+n

# 2024-12-21 22:24:43.635355
+no

# 2024-12-21 22:25:03.692550
+88134386c67ba882dc0a800d10aaa3e4759155fac3aeddf08025203a206f72e0 this is the docker container id. use it and run the shell

# 2024-12-21 22:25:56.373478
+y

# 2024-12-21 22:25:56.383070
+/run docker exec -it 88134386c67ba882dc0a800d10aaa3e4759155fac3aeddf08025203a206f72e0 /bin/bash

# 2024-12-21 22:25:56.383579
+/run docker exec -it 88134386c67ba882dc0a800d10aaa3e4759155fac3aeddf08025203a206f72e0 /bin/bash

# 2024-12-21 22:36:44.621136
+n

# 2024-12-21 22:39:28.794142
+home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/options.py:109 UserWarning: You have an incompatible version of 'pyarrow' installed (11.0.0), please install a version that adheres to: 'pyarrow<10.1.0,>=10.0.1; extra == "pandas"'

# 2024-12-21 22:41:37.940742
+y

# 2024-12-21 22:41:37.947756
+/run docker exec -it 88134386c67ba882dc0a800d10aaa3e4759155fac3aeddf08025203a206f72e0 pip show pyarrow

# 2024-12-21 22:41:37.947756
+/run docker exec -it 88134386c67ba882dc0a800d10aaa3e4759155fac3aeddf08025203a206f72e0 pip show pyarrow

# 2024-12-21 22:41:41.791357
+y

# 2024-12-21 22:41:55.511655
+/run docker exec -it 88134386c67ba882dc0a800d10aaa3e4759155fac3aeddf08025203a206f72e0 pip uninstall pyarrow

# 2024-12-21 22:41:55.511655
+/run docker exec -it 88134386c67ba882dc0a800d10aaa3e4759155fac3aeddf08025203a206f72e0 pip uninstall pyarrow

# 2024-12-21 22:41:59.183925
+n

# 2024-12-21 23:03:33.813949
+
+n
+ndocker-compose up --build -d using file

# 2024-12-21 23:03:36.442782
+n

# 2024-12-21 23:03:43.103924
+docker-compose up --build -d using file

# 2024-12-21 23:04:15.381343
+n

# 2024-12-21 23:06:06.202162
+how to delete all objects from docker

# 2024-12-21 23:06:32.883593
+y

# 2024-12-21 23:06:32.894138
+/run docker stop $(docker ps -aq)

# 2024-12-21 23:06:32.894138
+/run docker stop $(docker ps -aq)

# 2024-12-21 23:06:35.007073
+y

# 2024-12-21 23:06:41.549903
+/run docker rm $(docker ps -aq)

# 2024-12-21 23:06:41.549903
+/run docker rm $(docker ps -aq)

# 2024-12-21 23:06:44.770369
+y

# 2024-12-21 23:06:46.442978
+/run docker rmi $(docker images -q)

# 2024-12-21 23:06:46.442978
+/run docker rmi $(docker images -q)

# 2024-12-21 23:06:46.990336
+y
+y

# 2024-12-21 23:06:49.346012
+y

# 2024-12-21 23:06:50.401166
+/run docker volume rm $(docker volume ls -q)

# 2024-12-21 23:06:50.401166
+/run docker volume rm $(docker volume ls -q)

# 2024-12-21 23:06:51.258911
+y

# 2024-12-21 23:06:52.053220
+/run docker network rm $(docker network ls -q)

# 2024-12-21 23:06:52.053728
+/run docker network rm $(docker network ls -q)

# 2024-12-21 23:06:52.886034
+y

# 2024-12-21 23:07:33.427214
+/run docker stop $(docker ps --all --quiet)

# 2024-12-21 23:07:33.427214
+/run docker stop $(docker ps --all --quiet)

# 2024-12-21 23:07:34.819060
+y

# 2024-12-21 23:07:35.365520
+/run docker rm $(docker ps --all --quiet)

# 2024-12-21 23:07:35.365520
+/run docker rm $(docker ps --all --quiet)

# 2024-12-21 23:07:36.229188
+y

# 2024-12-21 23:07:36.972367
+/run docker rmi $(docker images --quiet)

# 2024-12-21 23:07:36.972367
+/run docker rmi $(docker images --quiet)

# 2024-12-21 23:07:37.857501
+y

# 2024-12-21 23:07:38.656355
+/run docker volume rm $(docker volume ls --quiet)

# 2024-12-21 23:07:38.656355
+/run docker volume rm $(docker volume ls --quiet)

# 2024-12-21 23:07:39.479309
+y

# 2024-12-21 23:07:40.735126
+/run docker network rm $(docker network ls --quiet)

# 2024-12-21 23:07:40.735126
+/run docker network rm $(docker network ls --quiet)

# 2024-12-21 23:07:41.590156
+y

# 2024-12-21 23:07:47.897712
+restart docker

# 2024-12-21 23:08:05.794919
+y

# 2024-12-21 23:08:05.801453
+/run Stop-Service com.docker.service

# 2024-12-21 23:08:05.801453
+/run Stop-Service com.docker.service

# 2024-12-21 23:18:00.016762
+n

# 2024-12-21 23:18:00.309795
+nn

# 2024-12-21 23:18:03.526333
+n

# 2024-12-22 00:38:22.787210
+nn
+n
+т

# 2024-12-22 00:38:25.410695
+т

# 2024-12-22 00:38:27.749853
+n

# 2024-12-22 00:38:57.691949
+AIRFLOW__WEBSERVER__SECRET_KEY: 'WEnN4kNtuSmRwJC6RZenXg=='n

# 2024-12-22 00:38:59.774010
+n

# 2024-12-22 00:39:12.872055
+find docker compose file

# 2024-12-22 00:39:26.460780
+n

# 2024-12-22 00:39:36.440266
+docker compose airflow i mean

# 2024-12-22 00:39:58.270036
+n

# 2024-12-22 00:40:12.083221
+do you see the file just yes or no

# 2024-12-22 00:40:40.272801
+reed it

# 2024-12-22 00:42:00.398438
+need to add secret key to all services AIRFLOW__WEBSERVER__SECRET_KEY: 'WEnN4kNtuSmRwJC6RZenXg=='

# 2024-12-22 01:03:22.789561
+what the demo.py dag does?

# 2024-12-22 01:04:02.753246
+i look into logs and don't see any output

# 2024-12-22 01:05:37.000109
+т

# 2024-12-22 01:05:39.284467
+n

# 2024-12-22 01:06:31.816727
+docker-compose -f docker-compose-airflow.yml logs -f

# 2024-12-22 01:06:46.265220
+n

# 2024-12-22 01:55:57.713726
+/add ibra.py

# 2024-12-22 01:56:07.217464
+n

# 2024-12-22 01:56:18.593211
+/add dags/ibra.py

# 2024-12-22 01:56:28.868415
+go through the dag

# 2024-12-22 01:57:57.965263
+what happened try again

# 2024-12-22 01:58:48.021076
+find ibra.py

# 2024-12-22 01:59:05.681908
+fix if needed

# 2024-12-22 01:59:43.429233
+try again

# 2024-12-22 02:00:20.599750
+don't change the file just write the code

# 2024-12-22 02:06:31.353375
+go through the demo.py and add logging as well

# 2024-12-22 02:07:16.955609
+just give code i will change myself

# 2024-12-22 02:09:47.837704
+fix if needed

# 2024-12-22 02:12:11.441358
+ibra.py shows in logs {ibra.py:7} INFO - Hello, Airflow! and demo.py shows {demo.py:15} INFO - *** why

# 2024-12-22 02:13:10.569475
+give full oce

# 2024-12-22 02:14:40.166267
+still same issue
